import cv2
import tflite_runtime.interpreter as tflite
import numpy as np
import math as m
import time
import os

# สร้าง directory สำหรับเก็บผลลัพธ์ ถ้ายังไม่มี
output_dir = '/home/pi/mediapipe_output'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

output_video_path = os.path.join(output_dir, 'posture_output_tflite.mp4')

# ฟังก์ชันสำหรับคำนวณระยะทาง
def findDistance(x1, y1, x2, y2):
    return m.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)

# ฟังก์ชันสำหรับคำนวณมุม
def findAngle(x1, y1, x2, y2):
    if y1 == 0:
        y1 = 1  # ป้องกันการหารด้วยศูนย์
    theta = m.acos((y2 - y1) * (-y1) / (m.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) * y1))
    return int(180 / m.pi * theta)

# ฟังก์ชันสำหรับส่งคำเตือน
def sendWarning():
    print("คำเตือน: อยู่ในท่าทางที่ไม่ดีนานเกินไป!")

# โหลดโมเดล TensorFlow Lite
model_path = 'lite-model_movenet_singlepose_lightning_3.tflite'  # เปลี่ยนเป็น path ของโมเดล .tflite ของคุณ
interpreter = tflite.Interpreter(model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# เปิดกล้อง (Camera Module 3)
cap = cv2.VideoCapture(0)

# ตั้งค่าขนาดเฟรมและ fps ของวิดีโอ
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
fps = 15

# สร้าง VideoWriter
video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))

good_frames, bad_frames = 0, 0
font = cv2.FONT_HERSHEY_SIMPLEX

print("เริ่มการตรวจจับท่าทาง...")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    h, w = frame.shape[:2]
    # ปรับขนาดเฟรมสำหรับโมเดล
    resized_frame = cv2.resize(frame, (192, 192))  # ขนาด input ของ MoveNet Lightning คือ 192x192
    input_data = np.expand_dims(resized_frame, axis=0)
    input_data = (input_data / 255.0).astype(np.float32) # Normalize the input

    # กำหนด input ให้กับ interpreter
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # รัน inference
    interpreter.invoke()

    # ดึงผลลัพธ์ pose landmarks
    output_data = interpreter.get_tensor(output_details[0]['index'])
    keypoints = output_data[0][0]  # shape: (17, 3)  17 keypoints, each with y, x, confidence

    # วาดโครงกระดูกและคำนวณมุม
    if keypoints.any(): # Check if keypoints are detected
        # Scale keypoint coordinates to the original frame size
        def scale_keypoint(kp, original_width, original_height, model_input_size=192):
            return int(kp[1] * original_width), int(kp[0] * original_height)

        l_shldr_x, l_shldr_y = scale_keypoint(keypoints[5], w, h)
        r_shldr_x, r_shldr_y = scale_keypoint(keypoints[6], w, h)
        l_ear_x, l_ear_y = scale_keypoint(keypoints[3], w, h) # Changed from keypoint 0 to 3 (ear)
        l_hip_x, l_hip_y = scale_keypoint(keypoints[11], w, h)

        # Draw circles for the keypoints.  Make them smaller.
        cv2.circle(frame, (l_shldr_x, l_shldr_y), 3, (0, 255, 0), -1)
        cv2.circle(frame, (r_shldr_x, r_shldr_y), 3, (0, 255, 0), -1)
        cv2.circle(frame, (l_ear_x, l_ear_y), 3, (0, 255, 0), -1)
        cv2.circle(frame, (l_hip_x, l_hip_y), 3, (0, 255, 0), -1)

        # Draw lines connecting the keypoints
        cv2.line(frame, (l_shldr_x, l_shldr_y), (r_shldr_x, r_shldr_y), (255, 255, 255), 2)
        cv2.line(frame, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), (255, 255, 255), 2)
        cv2.line(frame, (l_shldr_x, l_shldr_y), (l_hip_x, l_hip_y), (255, 255, 255), 2)

        offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)
        neck_angle = findAngle(l_shldr_x, l_shldr_y, l_ear_x, l_ear_y)
        torso_angle = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)

        if neck_angle < 40 and torso_angle < 10:
            bad_frames = 0
            good_frames += 1
            color = (0, 255, 0)
        else:
            good_frames = 0
            bad_frames += 1
            color = (0, 0, 255)

        cv2.putText(frame, f'Neck: {neck_angle}  Torso: {torso_angle}', (10, 30), font, 0.6, color, 2)
        time_good = good_frames / fps
        time_bad = bad_frames / fps

        if time_bad > 180:
            sendWarning()

        cv2.putText(frame, f'Time Good: {round(time_good,1)}s  Bad: {round(time_bad,1)}s', (10, h - 10), font, 0.6, color, 2)

    # บันทึกวิดีโอ
    video_writer.write(frame)

    # แสดงผล (ถ้าต่อหน้าจอ)
    # cv2.imshow("Posture Detection", frame)
    # if cv2.waitKey(1) & 0xFF == ord('q'):
    #     break

cap.release()
video_writer.release()
cv2.destroyAllWindows()
